{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Pytorch colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuirp6oEBD8OY4fYbOXIO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ram001code/Sentiment_Analysis_pytorch/blob/main/Sentiment_Analysis_Pytorch_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPXsIFiazIkw",
        "outputId": "40c6f567-7d0b-4548-b480-cd5827690546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "# from torchtext import data\n"
      ],
      "metadata": {
        "id": "xBb7nbWwzJbD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "6j09KJd7zJdp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class torchtext.data.Field(sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
        "\n",
        "# TDefines a datatype together with instructions for converting to Tensor.\n",
        "\n",
        "Field class models common text processing datatypes that can be represented by tensors. It holds a Vocab object that defines the set of possible values for elements of the field and their corresponding numerical representations.\n",
        "\n",
        "```\n",
        "\n",
        "tokenize – The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split.\n",
        "\n",
        "tokenizer_language – The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.\n",
        "\n",
        "include_lengths – Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n"
      ],
      "metadata": {
        "id": "9j5SpnyZ0Us7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pytorch torchvision -c pytorch\n",
        "\n",
        "!pip install torchtext==0.10.0      #Maybe legacy was removed in version 0.11.0. so we downgraded to 0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gm4X1Ab2tCZ",
        "outputId": "42e11635-cb7a-47f4-b7bf-65e3aa7c3063"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The legacy components are placed in torchtext.legacy.data folder as follows:\n",
        "\n",
        "\n",
        "torchtext.data.Pipeline -> torchtext.legacy.data.Pipeline\n",
        "\n",
        "torchtext.data.Batch -> torchtext.legacy.data.Batch\n",
        "\n",
        "torchtext.data.Example -> torchtext.legacy.data.Example\n",
        "\n",
        "torchtext.data.Field -> torchtext.legacy.data.Field\n",
        "\n",
        "torchtext.data.Iterator -> torchtext.legacy.data.Iterator\n",
        "\n",
        "torchtext.data.Dataset -> torchtext.legacy.data.Dataset\n"
      ],
      "metadata": {
        "id": "yHQwCGzz4epB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import data"
      ],
      "metadata": {
        "id": "kRWEhQA-2KhJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths = True)"
      ],
      "metadata": {
        "id": "_zKISNeXzJij"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets"
      ],
      "metadata": {
        "id": "JlwwX2um5tFt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "Here we have downloaded the imdb dataset for python sentiment analysis and divided it into train test and validation split. \n",
        "The dataset is already divided into a train and test set, we further create a validation set from it.\n",
        "\n",
        "We further limit the number of words the model will learn to 25000, this will choose the most used 25000 words from the dataset and use them for training. \n",
        "Significantly reducing the work of the model without any real loss in accuracy.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "labels = data.LabelField(dtype = torch.float)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(txt, labels)\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(seed))\n",
        "\n",
        "num_words = 25_000\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAWGrmTn2J-D",
        "outputId": "b02bb198-b7ed-4a4c-99ee-2fb87ff272e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 75.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "build_vocab(*args, **kwargs)\n",
        "\n",
        "Construct the Vocab object for this field from one or more datasets.\n",
        "\n",
        "Parameters:\t\n",
        "arguments (Positional) – Dataset objects or other iterable data sources from which to construct the Vocab object that \n",
        "represents the set of possible values for this field. If a Dataset object is provided, all columns corresponding to this field are used; \n",
        "individual columns can also be provided directly.\n",
        "\n",
        "keyword arguments (Remaining) – Passed to the constructor of Vocab.\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "txt.build_vocab(train_data, \n",
        "                 max_size = num_words, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "labels.build_vocab(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9BDZ7kGzJli",
        "outputId": "2ca76f1b-76d0-427a-f965-74cb8bf5a871"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29517.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We are now creating a training, testing and validation batch from the data that we have for preparing it to be fed to \n",
        "the model in the form of batches of 64 samples at a time. Reduce this if you get out of memory error.\n",
        "'''\n",
        "\n",
        "btch_size = 64\n",
        "\n",
        "train_itr, valid_itr, test_itr = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = btch_size,\n",
        "    sort_within_batch = True,\n",
        "    device = device)\n",
        "\n"
      ],
      "metadata": {
        "id": "-HBquRSrzJn6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the parameters for python sentiment analysis model and pass it to an instance of the model class we just defined. The number of input parameters, hidden layer, and the output dimension along with throughput rate and bidirectionality boolean is defined. We also pass the pad token index from the vocabulary that we created earlier."
      ],
      "metadata": {
        "id": "jpXw3X049Ojb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, word_limit, dimension_embedding, dimension_hidden, dimension_output, num_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(word_limit, dimension_embedding, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(dimension_embedding, \n",
        "                           dimension_hidden, \n",
        "                           num_layers=num_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(dimension_hidden * 2, dimension_output)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, len_txt):\n",
        "        \n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "               \n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, len_txt.to('cpu'))\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                            \n",
        "        return self.fc(hidden)\n",
        "\n"
      ],
      "metadata": {
        "id": "JlfHQu8MzJyc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we print some details about our model. Getting the number of trainable parameters that are present there in the model.\n",
        "\n",
        "We then get the pre-trained embedding weights and copy them to our model so that it does not need to learn the embeddings, and can directly focus on the job at hand that is learning the sentiments related to those embeddings.\n",
        "\n",
        "Pretrained embedding weights are placed in place of the initial ones."
      ],
      "metadata": {
        "id": "9o3GaysD9m_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension_input = len(txt.vocab)\n",
        "dimension_embedding = 100\n",
        "dimension_hddn = 256\n",
        "dimension_out = 1\n",
        "layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "idx_pad = txt.vocab.stoi[txt.pad_token]\n",
        "\n",
        "\n",
        "model = RNN(dimension_input, \n",
        "            dimension_embedding, \n",
        "            dimension_hddn, \n",
        "            dimension_out, \n",
        "            layers, \n",
        "            bidirectional, \n",
        "            dropout, \n",
        "            idx_pad)"
      ],
      "metadata": {
        "id": "sHnmQL9CzJ1b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        " \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "pretrained_embeddings = txt.vocab.vectors\n",
        " \n",
        "print(pretrained_embeddings.shape)\n",
        "unique_id = txt.vocab.stoi[txt.unk_token]\n",
        " \n",
        "model.embedding.weight.data[unique_id] = torch.zeros(dimension_embedding)\n",
        "model.embedding.weight.data[idx_pad] = torch.zeros(dimension_embedding)\n",
        " \n",
        "print(model.embedding.weight.data)"
      ],
      "metadata": {
        "id": "G_ZdDICIzJ4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1207fd-e8bb-4e8f-ba9e-2e6663b37ed2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 4,810,857 trainable parameters\n",
            "torch.Size([25002, 100])\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.3603,  1.1402, -1.0729,  ..., -0.0867, -0.9023, -0.9291],\n",
            "        ...,\n",
            "        [-0.6512, -0.2244, -0.3158,  ..., -1.5751,  1.8184,  0.0519],\n",
            "        [ 0.8822, -0.6750, -0.6353,  ...,  1.4760, -1.5389, -0.0588],\n",
            "        [ 0.7433,  0.7861,  1.1492,  ..., -0.4720, -1.1798,  0.7291]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking above steps in details "
      ],
      "metadata": {
        "id": "PPd9v8D1ARc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = txt.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "metadata": {
        "id": "0v7yEt5rzJ7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba6dc92-b466-479f-d133-a52b185c6041"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25002, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "metadata": {
        "id": "LqMO0lf_zJ9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017cc303-21f2-44c0-dd1f-d8081ec21dfe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.1219,  0.7036,  0.0626,  ...,  0.1124,  0.2100, -0.1781],\n",
              "        [-0.5005,  0.2531,  1.8303,  ...,  0.5205,  0.6098,  0.8963],\n",
              "        [-1.8683,  0.1008, -0.1267,  ...,  0.3982,  0.7209,  0.0699]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_id = txt.vocab.stoi[txt.unk_token]\n",
        " \n",
        "model.embedding.weight.data[unique_id] = torch.zeros(dimension_embedding)\n",
        "model.embedding.weight.data[idx_pad] = torch.zeros(dimension_embedding)\n",
        " \n",
        "print(model.embedding.weight.data)"
      ],
      "metadata": {
        "id": "hizwfLuLzJ_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b79580-6599-4865-f107-e848f2212336"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.1219,  0.7036,  0.0626,  ...,  0.1124,  0.2100, -0.1781],\n",
            "        [-0.5005,  0.2531,  1.8303,  ...,  0.5205,  0.6098,  0.8963],\n",
            "        [-1.8683,  0.1008, -0.1267,  ...,  0.3982,  0.7209,  0.0699]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now we define some parameters regarding the model, that is the optimizer we are going to use and the criterion of loss we need.\n",
        "We chose adam optimizer for fast convergence of the model along with logistic loss function. \n",
        "We place the model and the criterion on the gpu.\n",
        "'''\n",
        "\n",
        "import torch.optim as optim\n",
        " \n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        " \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "vWjHdbYUAZd8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of the model"
      ],
      "metadata": {
        "id": "r1TtZJuQA0Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "We now begin the necessary functions for training and evaluation of sentiment analysis model.\n",
        "\n",
        "The first one being the binary accuracy function, which we’ll use for getting the accuracy of the model each time.\n",
        "'''\n",
        "\n",
        "def bin_acc(preds, y):\n",
        "   \n",
        "    predictions = torch.round(torch.sigmoid(preds))\n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "SNP1ZXtfAZgx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We define the function for training and evaluating the models. The process here is standard. \n",
        "We start by looping through the number of epochs and the number of iterations in each epoch is according to the batch size that we defined.\n",
        " We pass the text to the model, get the predictions from it, calculate the loss for each iteration and then backward propagate that loss.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def train(model, itr, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for i in itr:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, len_txt = i.text\n",
        "        \n",
        "        predictions = model(text, len_txt).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, i.label)\n",
        "        \n",
        "        acc = bin_acc(predictions, i.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(itr), epoch_acc / len(itr)\n",
        " \n",
        "def evaluate(model, itr, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in itr:\n",
        " \n",
        "            text, len_txt = i.text\n",
        "            \n",
        "            predictions = model(text, len_txt).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, i.label)\n",
        "            \n",
        "            acc = bin_acc(predictions, i.label)\n",
        " \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(itr), epoch_acc / len(itr)"
      ],
      "metadata": {
        "id": "3oaKWaOfAZkT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We build a helper function epoch_time for calculating the time each epoch takes to complete its run and print it. \n",
        "We set the number of epochs to 5 and then begin our training. Adding the training and validation loss at each stage, if we need to \n",
        "understand or plot the training curve at a later point. We save the python sentiment analysis model that has the best validation loss.\n",
        "'''\n",
        "\n",
        "\n",
        "import time\n",
        " \n",
        "def epoch_time(start_time, end_time):\n",
        "    used_time = end_time - start_time\n",
        "    used_mins = int(used_time / 60)\n",
        "    used_secs = int(used_time - (used_mins * 60))\n",
        "    return used_mins, used_secs\n",
        "num_epochs = 5\n",
        " \n",
        "best_valid_loss = float('inf')\n",
        " \n",
        "for epoch in range(num_epochs):\n",
        " \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_itr, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_itr, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        " \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ju7AV8kAZnT",
        "outputId": "8bb741ac-08bf-4e42-cb50-bde775195ebc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.671 | Train Acc: 58.60%\n",
            "\t Val. Loss: 0.581 |  Val. Acc: 69.89%\n",
            "Epoch: 02 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.598 | Train Acc: 68.07%\n",
            "\t Val. Loss: 0.467 |  Val. Acc: 79.09%\n",
            "Epoch: 03 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.459 | Train Acc: 79.00%\n",
            "\t Val. Loss: 0.377 |  Val. Acc: 84.01%\n",
            "Epoch: 04 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.370 | Train Acc: 84.15%\n",
            "\t Val. Loss: 0.321 |  Val. Acc: 86.75%\n",
            "Epoch: 05 | Epoch Time: 0m 37s\n",
            "\tTrain Loss: 0.298 | Train Acc: 88.09%\n",
            "\t Val. Loss: 0.385 |  Val. Acc: 82.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "cokDsT7RPosS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        " \n",
        "test_loss, test_acc = evaluate(model, test_itr, criterion)\n",
        " \n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4qNG2ReBkEa",
        "outputId": "1cac1607-4f24-448d-f1a5-59f894e9d2eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.332 | Test Acc: 86.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We can also check the model on our data. This is trained to classify the movie reviews into positive, negative, and neutral, \n",
        "therefore we will pass to it relatable data for checking. So for that we will import and load spacy for tokenizing the data \n",
        "we need to give to the model. In the beginning, while defining the preprocessing we used spacy built-in torch.text, but here \n",
        "we are not using batches, and the preprocessing that we need to do can be handled by the spacy library. We define a predict sentiment\n",
        " function for this. After the preprocessing, we convert it into tensors and ready to be passed to the model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        " \n",
        "def pred(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [txt.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "metadata": {
        "id": "oftVpeiMBkIJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We define another helper function that will print the sentiment of the comment based on the score that the model provides.\n",
        "'''\n",
        "\n",
        "sent=[\"positive\",\"neutral\",\"negative\"]\n",
        "def print_sent(x):\n",
        "  if (x<0.3): print(sent[0])\n",
        "  elif (x>0.3 and x<0.7): print(sent[1])\n",
        "  else: print(sent[2])\n"
      ],
      "metadata": {
        "id": "qQ4DjrnoBkMk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just pass any data and test what does the model think about it"
      ],
      "metadata": {
        "id": "Dbo5AdwGQfS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_sent(pred(model, \"This film was great\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd85gVZUQSuF",
        "outputId": "7c3e639d-ebfd-46e6-cd8d-2e121a653e82"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sent(pred(model, \"This was the best movie i have seen in a while. The cast was great and the script was awesome, and the direction just blew my mind\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekpqiLArQSxa",
        "outputId": "4222737f-8339-477f-c729-f8279a1bcdd6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sent(pred(model, \"This film is horrible\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGo4ZgkyQS0i",
        "outputId": "e40fc208-e39b-47cb-b5f9-5fef4b5c1bb5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sent(pred(model, \"the cast was dumb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRNQJTqhQS3X",
        "outputId": "53019e98-933a-417f-f045-06d60afc6597"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_sent(pred(model, \"Why does this fil exist\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmNBk8Q5QS-f",
        "outputId": "d3b9c6bf-eda0-4600-d5f8-e38f4ba95ba3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    }
  ]
}